{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\macsd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\macsd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\macsd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#for word embeddindg\n",
    "#import gensim\n",
    "#from gensim.models import Word2Vec\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./tagged_db.pickle', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'poco'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(dataset)\n",
    "#df.set_index(name, drop=True, append=False, inplace=False, verify_integrity=False)\n",
    "df = df.transpose()\n",
    "df['new_col'] = range(1, len(df) + 1)\n",
    "df = df.set_index(\"new_col\", drop=True, append=False, inplace=False, verify_integrity=False)\n",
    "df = df.drop(df[df.topic == \"filosofia\"].index)\n",
    "df = df.drop(df[df.topic == \"geologia\"].index)\n",
    "df = df.drop(df[df.topic == \"informatica\"].index)\n",
    "df = df.drop(df[df.topic == \"biologia\"].index)\n",
    "df = df.drop(df[df.topic == \"nessuno\"].index)\n",
    "df.wordlist[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "       row.wordlist =  ' '.join(row.wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text=text.strip()  \n",
    "    text=re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text) \n",
    "    return text\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('italiano')]\n",
    "    return ' '.join(a)\n",
    "#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>wordlist</th>\n",
       "      <th>topic</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_col</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001.jpg</td>\n",
       "      <td>esercizio sale poco solubile determinare quali...</td>\n",
       "      <td>chimica</td>\n",
       "      <td>esercizio sale poco solubile determinare quali...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000003.jpg</td>\n",
       "      <td>denotare effettivamente qualche non chiaro tal...</td>\n",
       "      <td>matematica</td>\n",
       "      <td>denotare effettivamente qualche non chiaro tal...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000005.jpg</td>\n",
       "      <td>quanti domini elettronici dipende geometria qu...</td>\n",
       "      <td>chimica</td>\n",
       "      <td>quanti domini elettronici dipende geometria qu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>000008.jpg</td>\n",
       "      <td>una sostanza costituita idrogeno azoto dato al...</td>\n",
       "      <td>chimica</td>\n",
       "      <td>una sostanza costituita idrogeno azoto dato al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>000026.jpg</td>\n",
       "      <td>l'analisi una sostanza costituita azoto cloro ...</td>\n",
       "      <td>chimica</td>\n",
       "      <td>l analisi una sostanza costituita azoto cloro ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           filename                                           wordlist  \\\n",
       "new_col                                                                  \n",
       "1        000001.jpg  esercizio sale poco solubile determinare quali...   \n",
       "2        000003.jpg  denotare effettivamente qualche non chiaro tal...   \n",
       "3        000005.jpg  quanti domini elettronici dipende geometria qu...   \n",
       "6        000008.jpg  una sostanza costituita idrogeno azoto dato al...   \n",
       "8        000026.jpg  l'analisi una sostanza costituita azoto cloro ...   \n",
       "\n",
       "              topic                                         clean_text  y  \n",
       "new_col                                                                    \n",
       "1           chimica  esercizio sale poco solubile determinare quali...  0  \n",
       "2        matematica  denotare effettivamente qualche non chiaro tal...  2  \n",
       "3           chimica  quanti domini elettronici dipende geometria qu...  0  \n",
       "6           chimica  una sostanza costituita idrogeno azoto dato al...  0  \n",
       "8           chimica  l analisi una sostanza costituita azoto cloro ...  0  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def finalpreprocess(string):\n",
    "    return lemmatizer(preprocess(string))\n",
    "df['clean_text'] = df['wordlist'].apply(lambda x: finalpreprocess(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "a = le.fit_transform(df.topic)\n",
    "df[\"y\"] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"clean_text\"],df[\"y\"],test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.74      0.79        66\n",
      "           1       0.89      0.80      0.85       189\n",
      "           2       0.84      0.93      0.88       260\n",
      "\n",
      "    accuracy                           0.86       515\n",
      "   macro avg       0.86      0.82      0.84       515\n",
      "weighted avg       0.86      0.86      0.86       515\n",
      "\n",
      "Confusion Matrix: [[ 49   4  13]\n",
      " [  4 152  33]\n",
      " [  5  14 241]]\n"
     ]
    }
   ],
   "source": [
    "lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)  \n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = lr_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.61      0.74        66\n",
      "           1       0.94      0.76      0.84       189\n",
      "           2       0.79      0.97      0.87       260\n",
      "\n",
      "    accuracy                           0.84       515\n",
      "   macro avg       0.89      0.78      0.82       515\n",
      "weighted avg       0.86      0.84      0.84       515\n",
      "\n",
      "Confusion Matrix: [[ 40   2  24]\n",
      " [  1 144  44]\n",
      " [  1   8 251]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(X_train_vectors_tfidf, y_train)\n",
    "y_predict = clf.predict(X_test_vectors_tfidf)\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.35      0.51        66\n",
      "           1       0.84      0.83      0.83       189\n",
      "           2       0.80      0.93      0.86       260\n",
      "\n",
      "    accuracy                           0.82       515\n",
      "   macro avg       0.87      0.70      0.74       515\n",
      "weighted avg       0.83      0.82      0.81       515\n",
      "\n",
      "Confusion Matrix: [[ 23  14  29]\n",
      " [  1 157  31]\n",
      " [  0  17 243]]\n"
     ]
    }
   ],
   "source": [
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(X_train_vectors_tfidf, y_train)  \n",
    "#Predict y value for test dataset\n",
    "y_predict = nb_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = nb_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2056, 7585)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectors_tfidf.toarray().shape\n",
    "#y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=7585, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(7585, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64,64)\n",
    "        self.fc5 = nn.Linear(64,3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-209-903ce32d9a2b>:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_train = torch.from_numpy(y_train.values.astype(np.float))\n",
      "<ipython-input-209-903ce32d9a2b>:4: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_test = torch.Tensor((y_test.values.astype(np.float)))\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.from_numpy(X_train_vectors_tfidf.toarray())\n",
    "X_test = torch.Tensor(X_test_vectors_tfidf.toarray())\n",
    "y_train = torch.from_numpy(y_train.values.astype(np.float))\n",
    "y_test = torch.Tensor((y_test.values.astype(np.float)))\n",
    "training_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "testing_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=256, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(testing_dataset, batch_size = 256, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr= 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-202-8e3d07ae977f>:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.9344, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6702, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1681, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9354, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6539, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5062, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4660, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4542, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4455, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4383, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4326, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4265, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4200, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4122, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4043, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3946, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3829, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3687, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3482, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3254, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3060, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2842, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2399, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2054, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1768, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1514, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1121, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0883, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0854, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0492, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0312, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0207, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0151, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0092, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0073, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0063, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0050, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0042, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0035, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0030, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0026, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0023, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0020, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0017, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0015, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0013, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0012, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0010, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0009, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0008, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0008, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0008, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0007, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0008, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0007, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0007, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0006, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "tensor(9.7064e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(9.0257e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(8.5594e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(8.0083e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(7.5852e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(7.2023e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(6.8582e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(6.5722e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(6.2236e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(5.9763e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(5.7021e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4995e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2522e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0839e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8604e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7010e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5520e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4030e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2868e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1437e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0156e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8726e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7415e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6118e-05, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "lo = []\n",
    "for epoch in range(100):\n",
    "    for data in train_loader:\n",
    "        x, y = data\n",
    "        net.zero_grad()\n",
    "        x = x.view(-1,7585).float()\n",
    "        output = net(x)\n",
    "        loss = loss_function(output, y.type(torch.LongTensor)) # calc and grab the loss value\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)\n",
    "    lo.append(loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-202-8e3d07ae977f>:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1,7585))\n",
    "        #print(output)\n",
    "        for idx, i in enumerate(output):\n",
    "            #print(torch.argmax(i), y[idx])\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
